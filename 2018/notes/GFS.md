# Google File System

GFS是Google为其内部应用设计的分布式存储系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，并可以容错，给大量的用户提供总体性能较高的服务。

## GFS 结构

### 多个chunkserver
GFS中数据存储的单元结构是chunk，一个chunk在整个GFS系统上有3个副本，每个chunk副本相当于一个Linux文件，大小一般是64MB。同一个chunk的不同副本一般不会在同一台机器上（鸡蛋不能放在同一个篮子里）。每个chunk在创建时会被分配一个chunk句柄，chunk句柄是一个不变的、全局唯一的64位的ID。

一个chunkserver就是GFS系统中存放chunk副本的机器，GFS一般有很多chunkserver，chunk副本就是chunkserver磁盘上的文件，chunkserver管理这些文件，进行创建，读写，删除。

注意chunkserver和chunk是多对多的关系，一个chunkserver上有多个chunk的副本，同一个chunk的副本分布在不同的chunkserver上。

### 单点Master 

GFS只有一个Master（虽然有阴影master，在宕机时提供只读服务，但是阴影master不负责分布式调度），进行元数据总控和分布式调度。

元数据就是包括命名空间，访问控制信息，从文件到chunk的映射，和chunk副本位置等文件信息数据。元数据存储在master的内存上。（我第一次看到的时候被惊呆了！内存上难道不破丢失吗！？）

为什么元数据要存在内存上？

1）在内存上存取速度快，提高了master性能。

2）元数据本身不多。一个chunk的size是64MB，较大，元数据主要是chunk的信息，chunk越大，数量就越少，元数据就越少，所以存在内存上也是可以的。

3）内存中的元数据会被定时保存到磁盘上，并备份到多个机器上，不破丢失。备份主要是靠 记录操作日志 + 定时存档 的方式，每次存档之后，操作日志被清零，防止日志越来越大。

### 客户端

客户端提供类POSIX文件接口（没实现标准的posix），让用户能像操作本地文件一样操作GFS中的文件，如create，open，read，write，delete等。除了常规操作，GFS还提供快照和record append操作。快照可以用很低的花费为一个文件或者整个目录树创建一个副本。record append允许多个客户端并发的append数据到同一个文件，并且是原子性操作。

应用程序使用客户端与GFS系统交互。

## GFS 运作

![](http://ohr9krjig.bkt.clouddn.com/GFSScreen%20Shot%202018-10-16%20at%2019.38.45.png)

GFS的三部分是如何运作的？

当一个用户想对某个文件进行读写操作时，GFS内部是怎么操作的？

1）客户端将应用程序指定的文件名和字节偏移量翻译为一个GFS文件及内部chunk序号，随后将它们作为参数，发送请求到master。

2）master找到对应的chunk句柄和副本位置，回复给客户端。

3）客户端缓存这些信息，使用GFS文件名+chunk序号作为key，客户端再与chunkserver交互，完成读写操作（写操作比读操作要复杂，因为写操作要更新全部副本，读只只需要从一个副本读即可）。

### 一致性

分布式系统很重要的一点是保持比不同的副本的一致性， 写操作，失败可能会引起多个副本的不一致；读操作，在不同的副本的上读到的内容可能不一致。

GFS定义了一些状态：

![](http://ohr9krjig.bkt.clouddn.com/GFS2Screen%20Shot%202018-10-16%20at%2019.58.48.png)

defined：客户端的数据完整写入

undefined：客户端的数据没有完整写入

consistent：同一个文件的多个副本是完全一致的

inconsisent：同一个文件的多个副本是不一致

可以看到 write和 append操作 失败后的状态是inconsistent。

串行 write 成功之后是 defined，并行write 成功之后是 consistent but undefined（因为不能保证客户端的数据被完整写入，写入的数据可能会被并行的write覆盖，多个客户端争夺在同一个文件write，形成交织写。但是因为写入成功，所以多副本数据必然一致。所有的副本的数据是一样的）。

串行append和并行append 成功之后的状态都 defined interspersed with inconsistent。append操作时，chunk主副本根据当前文件大小决定写入offset，客户端能够根据offset确切知道写入结果，无论是串行写入还是并发写入，其行为是defined。（为什么会不一致，有疑问？）

可以看到并行很有会造成不一致性的问题。
解决方法是：串行（干脆就不要并行），但是为了提升性能，可以减小串行的粒度（这里主要由应用程序来实现）。

### GFS 租约

GFS在chunk多副本之间选择出一个主副本，由主副本来协调客户端的写入，保证多副本之间维持一个全局统一的更新顺序，GFS使用了租约。

（租约可以减少客户端与master的交互，master只有一个，但是chunk副本有多个，客户端从master获得chunk信息，master选出主副本，主副本负责后续写入与客户端的交互，并维持多副本的一致性。）

数据写入流程：

![](http://ohr9krjig.bkt.clouddn.com/Screen%20Shot%202018-10-16%20at%2020.31.22.png)

1. 客户端要对某chunk执行操作，它询问master哪个chunkserver持有其租赁以及各副本的位置信息。如果没有任何人拿到租赁，master选择一个副本授予其租赁。
2. master将首要者、副本位置信息回复到客户端。客户端缓存这些数据以便未来重用，这样它仅需要在当前首要副本无法访问或者卸任时去再次联系master。
3. 客户端推送数据到所有的副本。只是推送，不会实施。
4. 一旦所有副本都确认收到了数据，客户端正式发送一个写请求到首要副本。写请求无真实数据，只是指令。首要副本会为每个请求分配唯一的递增序号，它也会严格按照此顺序来在本地状态中实施变异。
5. 首要将写请求推送到所有次级副本（请求中已带有分配的序号），每个次级副本都会严格按顺序依次实施变异。此时，真正实施了写操作。
6. 次级副本回复给首要的，确认他们已完成操作。
7. 首要副本回复客户端。在任何副本遭遇的任何错误，都被汇报给客户端。

数据推送：该过程仅仅是将数据推送给chunkserver并保存在内存缓冲区中，因此，无需保证数据推送顺序。

同步数据：将chunkserver内存缓冲区中数据存到磁盘上。需要保证多副本上数据写入序列的一致性，否则会出现副本之间数据不一致（因为可能有来自多个客户端的写请求，写入顺序不一样会导致不一致性），该指令由主副本来确定数据更新顺序然后将该顺序通知给其他从副本。

可以看到在写入的过程中，除了第一次，客户端是和master交互外，其余，客户端都是与主要副本交互，减轻了单点master的压力。

